---
title: action.start
sidebar_label: action.start
slug: /swml/methods/live_transcribe/action/start
description: Start a live transcription session.
tags: ['swml']
---

import APIField from "@site/src/components/APIField";

# action.start

Start a live translation session.

<APIField
  name="action.start"
  type="object"
  required={true}
>
  An object that contains the [`start parameters`](#start-parameters).
</APIField>

## **start Parameters**

<APIField
  name="start.webhook"
  type="string"
>
  The webhook URI to be called. Authentication can also be set in the url in the format of `username:password@url`.
</APIField>

<APIField
  name="start.lang"
  type="string"
  required={true}
  default="en"
>
  The language to transcribe.<br/>Learn more about our supported Voices & Languages [here](/voice/getting-started/voice-and-languages).
</APIField>

<APIField
  name="start.live_events"
  type="boolean"
  default="false"
>
  Whether to enable live events.
</APIField>

<APIField
  name="start.ai_summary"
  type="boolean"
  default="false"
>
  Whether to enable automatic AI summarization.
  When enabled, an AI-generated summary of the conversation will be sent to your webhook when the transcription session ends.
</APIField>

<APIField
  name="start.speech_timeout"
  type="integer"
  default="60000"
>
  The timeout for speech recognition.<br/>**Possible Values:** [`Minimum value: 1500`, `Maximum Value: None`]
</APIField>

<APIField
  name="start.vad_silence_ms"
  type="integer"
  default="300 | 500"
>
  Voice activity detection silence time in milliseconds.
  Default depends on the speech engine: `300` for Deepgram, `500` for Google.<br/>**Possible Values:** [`Minimum value: 1`, `Maximum Value: None`]
</APIField>

<APIField
  name="start.vad_thresh"
  type="integer"
  default="400"
>
  Voice activity detection threshold.<br/>**Possible Values:** [`Minimum value: 0`, `Maximum Value: 1800`]
</APIField>

<APIField
  name="start.debug_level"
  type="integer"
  default="0"
>
  Debug level for logging.
</APIField>

<APIField
  name="start.direction"
  type="[]"
  required={true}
  default="local-caller"
>
  The direction of the call that should be transcribed.<br/>**Possible Values:** [`remote-caller`, `local-caller`]
</APIField>

<APIField
  name="start.speech_engine"
  type="string"
  default="deepgram"
>
  The speech recognition engine to use.<br/>**Possible Values:** [`deepgram`, `google`]
</APIField>

<APIField
  name="start.ai_summary_prompt"
  type="string"
>
  The AI prompt that instructs how to summarize the conversation when `ai_summary` is enabled.
  This prompt is sent to an AI model to guide how it generates the summary.

  Example: "Summarize the key points and action items from this conversation."
</APIField>

## **Example**

```yaml andJson
live_transcribe:
  action:
    start:
      webhook: 'https://example.com/webhook'
      lang: en
      live_events: true
      ai_summary: true
      ai_summary_prompt: Summarize this conversation
      speech_timeout: 60000
      vad_silence_ms: 500
      vad_thresh: 400
      debug_level: 0
      direction:
        - remote-caller
        - local-caller
      speech_engine: deepgram
```
