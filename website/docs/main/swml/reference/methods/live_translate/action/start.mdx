---
title: action.start
sidebar_label: action.start
slug: /swml/methods/live_translate/action/start
description: Start a live translation session.
tags: ['swml']
---

import APIField from "@site/src/components/APIField";

# action.start

Start a live translation session.

<APIField
  name="action.start"
  type="object"
  required={true}
>
  An object that accepts the [`start parameters`](#start-parameters).
</APIField>

## **start Parameters**

<APIField
  name="start.webhook"
  type="string"
>
  The webhook URI to be called. Authentication can also be set in the url in the format of `username:password@url`.
</APIField>

<APIField
  name="start.from_lang"
  type="string"
  required={true}
>
  The language to translate from.<br/>Learn more about our supported Voices & Languages [here](/voice/getting-started/voice-and-languages).
</APIField>

<APIField
  name="start.to_lang"
  type="string"
  required={true}
>
  The language to translate to.<br/>Learn more about our supported Voices & Languages [here](/voice/getting-started/voice-and-languages).
</APIField>

<APIField
  name="start.from_voice"
  type="string"
  default="elevenlabs.josh"
>
  The TTS voice you want to use for the source language.<br/>Learn more about our supported Voices & Languages [here](/voice/getting-started/voice-and-languages).
</APIField>

<APIField
  name="start.to_voice"
  type="string"
  default="elevenlabs.josh"
>
  The TTS voice you want to use for the target language.<br/>Learn more about our supported Voices & Languages [here](/voice/getting-started/voice-and-languages).
</APIField>

<APIField
  name="start.live_events"
  type="boolean"
  default="false"
>
  Whether to enable live events.
</APIField>

<APIField
  name="start.ai_summary"
  type="boolean"
  default="false"
>
  Whether to enable AI summarization.
</APIField>

<APIField
  name="start.speech_timeout"
  type="integer"
  default="60000"
>
  The timeout for speech recognition.<br/>**Possible Values:** [`Minimum value: 1500`, `Maximum Value: None`]
</APIField>

<APIField
  name="start.vad_silence_ms"
  type="integer"
  default="300"
>
  Voice activity detection silence time in milliseconds.<br/>**Possible Values:** [`Minimum value: 1`, `Maximum Value: None`]
</APIField>

<APIField
  name="start.vad_thresh"
  type="integer"
  default="400"
>
  Voice activity detection threshold.<br/>**Possible Values:** [`Minimum value: 0`, `Maximum Value: 1800`]
</APIField>

<APIField
  name="start.debug_level"
  type="integer"
  default="0"
>
  Debug level for logging.
</APIField>

<APIField
  name="start.direction"
  type="string[]"
  required={true}
>
  The direction of the call that should be translated.<br/>**Possible Values:** [`remote-caller`, `local-caller`]
</APIField>

<APIField
  name="start.speech_engine"
  type="string"
  default="deepgram"
>
  The speech engine to use for transcription.<br/>**Possible Values:** [`deepgram`, `google`]
</APIField>

<APIField
  name="start.summary_prompt"
  type="string"
>
  The prompt for summarization.
</APIField>

## **Example**

```yaml andJson
live_translate:
  action:
    start:
      webhook: 'https://example.com/webhook'
      from_lang: en
      to_lang: es
      from_voice: en-US
      to_voice: es-ES
      live_events: true
      ai_summary: false
      speech_timeout: 30
      vad_silence_ms: 500
      vad_thresh: 0.6
      debug_level: 2
      direction:
        - remote-caller
        - local-caller
      speech_engine: default
      summary_prompt: Summarize this conversation
```
